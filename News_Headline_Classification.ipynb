{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83d\udcf0 News Headline Classification\n", "Multi-class classification using ML and Deep Learning."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n", "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n", "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n", "from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\n", "from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n", "from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n", "from wordcloud import WordCloud\nnltk.download('stopwords')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Load Dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_json('News_Category_Dataset_v3.json', lines=True)\n", "df = df[['headline', 'category']]\ndf.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Filter Categories"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["selected_categories = ['POLITICS','SPORTS','TECH','BUSINESS','ENTERTAINMENT']\n", "df = df[df['category'].isin(selected_categories)].reset_index(drop=True)\n", "df['category'].value_counts()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Text Preprocessing"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\n", "def clean_text(text):\n    text = text.lower()\n    text = re.sub(r'[^a-zA-Z ]','', text)\n    words = [w for w in text.split() if w not in stop_words]\n    return ' '.join(words)\n", "df['clean_headline'] = df['headline'].apply(clean_text)\ndf[['headline','clean_headline']].head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Label Encoding"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["le = LabelEncoder()\ndf['label'] = le.fit_transform(df['category'])\nle.classes_"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Train-Test Split"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(df['clean_headline'], df['label'], test_size=0.2, random_state=42, stratify=df['label'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# \ud83e\udd16 Model 1: TF-IDF + Logistic Regression"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tfidf = TfidfVectorizer(max_features=5000)\n", "X_train_tfidf = tfidf.fit_transform(X_train)\nX_test_tfidf = tfidf.transform(X_test)\n", "lr_model = LogisticRegression(max_iter=200)\nlr_model.fit(X_train_tfidf, y_train)\n", "y_pred_lr = lr_model.predict(X_test_tfidf)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# \ud83e\udde0 Model 2: LSTM"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X_train)\n", "X_train_seq = tokenizer.texts_to_sequences(X_train)\nX_test_seq = tokenizer.texts_to_sequences(X_test)\n", "X_train_pad = pad_sequences(X_train_seq, maxlen=30)\nX_test_pad = pad_sequences(X_test_seq, maxlen=30)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = Sequential()\nmodel.add(Embedding(input_dim=10000, output_dim=100, input_length=30))\n", "model.add(LSTM(128))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\n", "model.add(Dense(len(selected_categories), activation='softmax'))\n", "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["history = model.fit(X_train_pad, y_train, epochs=5, batch_size=64, validation_split=0.1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_pred_lstm = np.argmax(model.predict(X_test_pad), axis=1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# \ud83d\udcca Evaluation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('Logistic Regression Accuracy:', accuracy_score(y_test, y_pred_lr))\n", "print('LSTM Accuracy:', accuracy_score(y_test, y_pred_lstm))\n", "print('\\nLR Report\\n', classification_report(y_test, y_pred_lr, target_names=le.classes_))\n", "print('\\nLSTM Report\\n', classification_report(y_test, y_pred_lstm, target_names=le.classes_))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Confusion Matrix"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_test, y_pred_lr), annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\nplt.title('Logistic Regression Confusion Matrix')\nplt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_test, y_pred_lstm), annot=True, fmt='d', cmap='Greens', xticklabels=le.classes_, yticklabels=le.classes_)\nplt.title('LSTM Confusion Matrix')\nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# \u2601\ufe0f Word Clouds"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for cat in selected_categories:\n    text = ' '.join(df[df['category']==cat]['clean_headline'])\n    wc = WordCloud(width=800, height=400, background_color='white').generate(text)\n    plt.figure(figsize=(10,5))\n    plt.imshow(wc, interpolation='bilinear')\n    plt.title(cat)\n    plt.axis('off')\n    plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# \ud83d\udcc8 Model Comparison"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["models = ['Logistic Regression','LSTM']\naccuracies = [accuracy_score(y_test, y_pred_lr), accuracy_score(y_test, y_pred_lstm)]\nplt.bar(models, accuracies)\nplt.ylim(0,1)\nplt.title('Model Accuracy Comparison')\nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \u2705 Conclusion\nBoth models perform well. LSTM may capture context better, while Logistic Regression is faster and strong baseline."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 5}